import streamlit as st
import pandas as pd
import time
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By

# è¨­å®šç¶²é æ¨™é¡Œ
st.set_page_config(page_title="FB æŒ‰è®šæ•¸çµ±æ•´å·¥å…·", layout="wide")

st.title("ğŸ“Š FB å…¬é–‹è²¼æ–‡æŒ‰è®šæ•¸çµ±æ•´å™¨")
st.write("è«‹è¼¸å…¥ FB é€£çµï¼Œç³»çµ±å°‡æ¨¡æ“¬ç€è¦½å™¨æŠ“å–å…¬é–‹é¡¯ç¤ºçš„æŒ‰è®šæ•¸å­—ã€‚")

# å·¦å´è¼¸å…¥å€
with st.sidebar:
    st.header("è¼¸å…¥è¨­å®š")
    urls_input = st.text_area("è²¼å…¥ FB é€£çµ (æ¯è¡Œä¸€å€‹)", height=300)
    start_button = st.button("ğŸš€ é–‹å§‹æŠ“å–æ•¸æ“š")

# æŠ“å–é‚è¼¯
def scrape_fb(urls):
    results = []
    
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    # å½è£æˆä¸€èˆ¬ç€è¦½å™¨é¿é–‹éƒ¨åˆ†é˜»æ“‹
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
    
    try:
        # åœ¨ Streamlit Cloud ç’°å¢ƒä¸­ï¼Œchromedriver é€šå¸¸ä½æ–¼ /usr/bin/chromedriver
        service = Service("/usr/bin/chromedriver")
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        progress_bar = st.progress(0)
        for i, url in enumerate(urls):
            # ... æ¥ä¸‹ä¾†çš„æŠ“å–é‚è¼¯ä¿æŒä¸è®Š ...
            driver.get(url.replace("www.facebook.com", "m.facebook.com"))
            time.sleep(5)
            # ... æŠ“å–ä»£ç¢¼ ...
            url = url.strip()
            if not url: continue
            
            # è½‰æ›ç‚ºç§»å‹•ç‰ˆç¶²é å¢åŠ æˆåŠŸç‡
            m_url = url.replace("www.facebook.com", "m.facebook.com")
            driver.get(m_url)
            time.sleep(5) # ç­‰å¾…è¼‰å…¥
            
            try:
                # å–å¾—ç¶²é æºç¢¼ä¸¦å°‹æ‰¾æ•¸å­—
                page_text = driver.find_element(By.TAG_NAME, 'body').text
                # å°‹æ‰¾ã€Œæ•¸å­— + å€‹è®šã€æˆ–é¡ä¼¼çµæ§‹
                match = re.search(r'(\d+)\s*(å€‹è®š|äºº|æ¬¡è®š|reactions|likes)', page_text)
                likes = match.group(0) if match else "æ‰¾ä¸åˆ°(å¯èƒ½éœ€ç™»å…¥)"
            except:
                likes = "æŠ“å–éŒ¯èª¤"
            
            results.append({"é€£çµ": url, "æŒ‰è®šæ•¸": likes})
            
            # æ›´æ–°é€²åº¦æ¢
            progress_bar.progress((i + 1) / len(urls))
            
        driver.quit()
        return results
    except Exception as e:
        st.error(f"ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

# æŒ‰ä¸‹æŒ‰éˆ•å¾Œçš„åŸ·è¡Œå‹•ä½œ
if start_button and urls_input:
    url_list = urls_input.split('\n')
    with st.spinner('çˆ¬èŸ²åŸ·è¡Œä¸­ï¼Œè«‹ç¨å€™...'):
        data = scrape_fb(url_list)
        
    if data:
        df = pd.DataFrame(data)
        st.subheader("âœ… æŠ“å–çµæœ")
        st.dataframe(df, use_container_width=True)
        
        # è£½ä½œ Excel ä¸‹è¼‰æŒ‰éˆ•
        csv = df.to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰çµæœ (CSV)",
            data=csv,
            file_name="fb_likes_report.csv",
            mime="text/csv",
        )
elif start_button and not urls_input:
    st.warning("è«‹å…ˆè¼¸å…¥è‡³å°‘ä¸€å€‹é€£çµï¼")
